{"version":3,"sources":["../../src/redux/persist.ts"],"names":["getReduxCacheFolder","path","join","process","cwd","getWorkerSlicesFolder","reduxSharedFile","dir","reduxChunkedNodesFilePrefix","reduxChunkedPagesFilePrefix","reduxWorkerSlicesPrefix","readFromCache","slices","optionalPrefix","cacheFolder","v8","deserialize","obj","nodesChunks","map","file","nodes","concat","length","report","info","Map","pagesChunks","pages","guessSafeChunkSize","values","showMaxSizeWarning","nodesToTest","valueCount","step","Math","max","ceil","maxSize","i","size","serialize","warn","floor","prepareCacheFolder","targetDir","contents","nodesMap","undefined","pagesMap","env","GATSBY_EXPERIMENTAL_LMDB_STORE","dummyNode","id","parent","children","internal","type","contentDigest","counter","owner","__gatsby_resolved","fields","set","entries","chunkSize","chunks","slice","safelyRenameToBak","tmpSuffix","suffixCounter","bakName","writeToCache","tmpDir","os","tmpdir","reduxCacheFolder","e"],"mappings":";;;;;;;;;AAAA;;AACA;;AACA;;AACA;;AAeA;;AACA;;AACA;;AAGA,MAAMA,mBAAmB,GAAG,MAC1B;AACAC,cAAKC,IAAL,CAAUC,OAAO,CAACC,GAAR,EAAV,EAA0B,cAA1B,CAFF;;AAIA,MAAMC,qBAAqB,GAAG,MAC5B;AACAJ,cAAKC,IAAL,CAAUC,OAAO,CAACC,GAAR,EAAV,EAA0B,eAA1B,CAFF;;AAIA,SAASE,eAAT,CAAyBC,GAAzB,EAA8C;AAC5C,SAAON,cAAKC,IAAL,CAAUK,GAAV,EAAgB,kBAAhB,CAAP;AACD;;AACD,SAASC,2BAAT,CAAqCD,GAArC,EAA0D;AACxD,SAAON,cAAKC,IAAL,CAAUK,GAAV,EAAgB,mBAAhB,CAAP;AACD;;AACD,SAASE,2BAAT,CAAqCF,GAArC,EAA0D;AACxD,SAAON,cAAKC,IAAL,CAAUK,GAAV,EAAgB,mBAAhB,CAAP;AACD;;AACD,SAASG,uBAAT,CAAiCH,GAAjC,EAAsD;AACpD,SAAON,cAAKC,IAAL,CAAUK,GAAV,EAAgB,sBAAhB,CAAP;AACD;;AAEM,SAASI,aAAT,CACLC,MADK,EAELC,cAAsB,GAAI,EAFrB,EAG2B;AAChC;AACA;AACA;AACA;AACA;AAEA,MAAIC,WAAW,GAAGd,mBAAmB,EAArC;;AAEA,MAAIY,MAAJ,EAAY;AACVE,IAAAA,WAAW,GAAGT,qBAAqB,EAAnC;AAEA,WAAOU,WAAGC,WAAH,CACL,2BACEN,uBAAuB,CAACI,WAAD,CAAvB,GACG,GAAED,cAAe,GADpB,GAEE,0CAAoBD,MAApB,CAHJ,CADK,CAAP;AAOD;;AAED,QAAMK,GAAsB,GAAGF,WAAGC,WAAH,CAC7B,2BAAaV,eAAe,CAACQ,WAAD,CAA5B,CAD6B,CAA/B,CArBgC,CAyBhC;;;AACA,QAAMI,WAAW,GAAG,gBAClBV,2BAA2B,CAACM,WAAD,CAA3B,GAA4C,GAD1B,EAElBK,GAFkB,CAEdC,IAAI,IAAIL,WAAGC,WAAH,CAAe,2BAAaI,IAAb,CAAf,CAFM,CAApB;AAIA,QAAMC,KAAmC,GAAG,GAAGC,MAAH,CAAU,GAAGJ,WAAb,CAA5C;;AAEA,MAAI,CAACA,WAAW,CAACK,MAAjB,EAAyB;AACvBC,sBAAOC,IAAP,CACG,oLADH;;AAGA,WAAO,EAAP;AACD;;AAEDR,EAAAA,GAAG,CAACI,KAAJ,GAAY,IAAIK,GAAJ,CAAQL,KAAR,CAAZ,CAvCgC,CAyChC;;AACA,QAAMM,WAAW,GAAG,gBAClBlB,2BAA2B,CAACK,WAAD,CAA3B,GAA4C,GAD1B,EAElBK,GAFkB,CAEdC,IAAI,IAAIL,WAAGC,WAAH,CAAe,2BAAaI,IAAb,CAAf,CAFM,CAApB;AAIA,QAAMQ,KAAmC,GAAG,GAAGN,MAAH,CAAU,GAAGK,WAAb,CAA5C;AAEAV,EAAAA,GAAG,CAACW,KAAJ,GAAY,IAAIF,GAAJ,CAAQE,KAAR,CAAZ;AAEA,SAAOX,GAAP;AACD;;AAEM,SAASY,kBAAT,CACLC,MADK,EAELC,kBAA2B,GAAG,KAFzB,EAGG;AACR;AACA;AACA;AACA;AACA;AAEA,QAAMC,WAAW,GAAG,EAApB,CAPQ,CAOe;;AACvB,QAAMC,UAAU,GAAGH,MAAM,CAACP,MAA1B;AACA,QAAMW,IAAI,GAAGC,IAAI,CAACC,GAAL,CAAS,CAAT,EAAYD,IAAI,CAACE,IAAL,CAAUJ,UAAU,GAAGD,WAAvB,CAAZ,CAAb;AACA,MAAIM,OAAO,GAAG,CAAd;;AACA,OAAK,IAAIC,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGN,UAApB,EAAgCM,CAAC,IAAIL,IAArC,EAA2C;AACzC,UAAMM,IAAI,GAAGzB,WAAG0B,SAAH,CAAaX,MAAM,CAACS,CAAD,CAAnB,EAAwBhB,MAArC;;AACAe,IAAAA,OAAO,GAAGH,IAAI,CAACC,GAAL,CAASI,IAAT,EAAeF,OAAf,CAAV;AACD,GAdO,CAgBR;;;AACA,MAAIP,kBAAkB,IAAIO,OAAO,GAAG,MAApC,EAA4C;AAC1Cd,sBAAOkB,IAAP,CACG,uJADH;AAGD,GArBO,CAuBR;AACA;AACA;;;AACA,SAAOP,IAAI,CAACQ,KAAL,CAAY,MAAM,IAAN,GAAa,IAAb,GAAoB,IAArB,GAA6BL,OAAxC,CAAP;AACD;;AAED,SAASM,kBAAT,CACEC,SADF,EAEEC,QAFF,EAGQ;AACN;AACA;AACA,QAAMC,QAAQ,GAAGD,QAAQ,CAACzB,KAA1B;AACAyB,EAAAA,QAAQ,CAACzB,KAAT,GAAiB2B,SAAjB;AAEA,QAAMC,QAAQ,GAAGH,QAAQ,CAAClB,KAA1B;AACAkB,EAAAA,QAAQ,CAAClB,KAAT,GAAiBoB,SAAjB;AAEA,8BAAc1C,eAAe,CAACuC,SAAD,CAA7B,EAA0C9B,WAAG0B,SAAH,CAAaK,QAAb,CAA1C,EATM,CAWN;;AACAA,EAAAA,QAAQ,CAACzB,KAAT,GAAiB0B,QAAjB;AACAD,EAAAA,QAAQ,CAAClB,KAAT,GAAiBqB,QAAjB;;AAEA,MAAIF,QAAJ,EAAc;AACZ,QAAIA,QAAQ,CAACP,IAAT,KAAkB,CAAlB,IAAuBrC,OAAO,CAAC+C,GAAR,CAAYC,8BAAvC,EAAuE;AACrE;AACA;AACA;AACA;AACA;AACA,YAAMC,SAAsB,GAAG;AAC7BC,QAAAA,EAAE,EAAG,eADwB;AAE7BC,QAAAA,MAAM,EAAG,EAFoB;AAG7BC,QAAAA,QAAQ,EAAE,EAHmB;AAI7BC,QAAAA,QAAQ,EAAE;AACRC,UAAAA,IAAI,EAAG,WADC;AAERC,UAAAA,aAAa,EAAG,YAFR;AAGRC,UAAAA,OAAO,EAAE,CAHD;AAIRC,UAAAA,KAAK,EAAG;AAJA,SAJmB;AAU7BC,QAAAA,iBAAiB,EAAE,EAVU;AAW7BC,QAAAA,MAAM,EAAE;AAXqB,OAA/B;AAaAf,MAAAA,QAAQ,CAACgB,GAAT,CAAaX,SAAS,CAACC,EAAvB,EAA2BD,SAA3B;AACD,KArBW,CAsBZ;;;AACA,UAAMtB,MAAoC,GAAG,CAAC,GAAGiB,QAAQ,CAACiB,OAAT,EAAJ,CAA7C;AACA,UAAMC,SAAS,GAAGpC,kBAAkB,CAACC,MAAD,CAApC;AACA,UAAMoC,MAAM,GAAG/B,IAAI,CAACE,IAAL,CAAUP,MAAM,CAACP,MAAP,GAAgB0C,SAA1B,CAAf;;AAEA,SAAK,IAAI1B,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG2B,MAApB,EAA4B,EAAE3B,CAA9B,EAAiC;AAC/B,kCACE/B,2BAA2B,CAACqC,SAAD,CAA3B,GAAyCN,CAD3C,EAEExB,WAAG0B,SAAH,CAAaX,MAAM,CAACqC,KAAP,CAAa5B,CAAC,GAAG0B,SAAjB,EAA4B1B,CAAC,GAAG0B,SAAJ,GAAgBA,SAA5C,CAAb,CAFF;AAID;AACF;;AAED,MAAIhB,QAAJ,EAAc;AACZ;AACA,UAAMnB,MAAoC,GAAG,CAAC,GAAGmB,QAAQ,CAACe,OAAT,EAAJ,CAA7C;AACA,UAAMC,SAAS,GAAGpC,kBAAkB,CAACC,MAAD,EAAS,IAAT,CAApC;AACA,UAAMoC,MAAM,GAAG/B,IAAI,CAACE,IAAL,CAAUP,MAAM,CAACP,MAAP,GAAgB0C,SAA1B,CAAf;;AAEA,SAAK,IAAI1B,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAG2B,MAApB,EAA4B,EAAE3B,CAA9B,EAAiC;AAC/B,kCACE9B,2BAA2B,CAACoC,SAAD,CAA3B,GAAyCN,CAD3C,EAEExB,WAAG0B,SAAH,CAAaX,MAAM,CAACqC,KAAP,CAAa5B,CAAC,GAAG0B,SAAjB,EAA4B1B,CAAC,GAAG0B,SAAJ,GAAgBA,SAA5C,CAAb,CAFF;AAID;AACF;AACF;;AAED,SAASG,iBAAT,CAA2BtD,WAA3B,EAAwD;AACtD;AACA;AACA,QAAMuD,SAAS,GAAI,MAAnB;AACA,MAAIC,aAAa,GAAG,CAApB;AACA,MAAIC,OAAO,GAAGzD,WAAW,GAAGuD,SAA5B,CALsD,CAKhB;;AAEtC,SAAO,yBAAWE,OAAX,CAAP,EAA4B;AAC1B,MAAED,aAAF;AACAC,IAAAA,OAAO,GAAGzD,WAAW,GAAGuD,SAAd,GAA0BC,aAApC;AACD;;AACD,yBAASxD,WAAT,EAAsByD,OAAtB;AAEA,SAAOA,OAAP;AACD;;AAEM,SAASC,YAAT,CACL1B,QADK,EAELlC,MAFK,EAGLC,cAAsB,GAAI,EAHrB,EAIC;AACN;AACA;AACA;AACA,MAAID,MAAJ,EAAY;AACV,UAAME,WAAW,GAAGT,qBAAqB,EAAzC;AAEA,iCACEK,uBAAuB,CAACI,WAAD,CAAvB,GACG,GAAED,cAAe,GADpB,GAEE,0CAAoBD,MAApB,CAHJ,EAIEG,WAAG0B,SAAH,CAAaK,QAAb,CAJF;AAMA;AACD,GAdK,CAgBN;AACA;;;AAEA,QAAM2B,MAAM,GAAG,0BAAYxE,cAAKC,IAAL,CAAUwE,YAAGC,MAAH,EAAV,EAAwB,YAAxB,CAAZ,CAAf,CAnBM,CAmB2D;;AAEjE/B,EAAAA,kBAAkB,CAAC6B,MAAD,EAAS3B,QAAT,CAAlB,CArBM,CAuBN;AACA;AACA;;AAEA,QAAM8B,gBAAgB,GAAG5E,mBAAmB,EAA5C;AAEA,MAAIuE,OAAO,GAAI,EAAf;;AACA,MAAI,yBAAWK,gBAAX,CAAJ,EAAkC;AAChC;AACAL,IAAAA,OAAO,GAAGH,iBAAiB,CAACQ,gBAAD,CAA3B;AACD,GAjCK,CAmCN;;;AACA,yBAASH,MAAT,EAAiBG,gBAAjB,EApCM,CAsCN;;AACA,MAAI;AACF,QAAIL,OAAO,KAAM,EAAjB,EAAoB;AAClB,+BAAWA,OAAX;AACD;AACF,GAJD,CAIE,OAAOM,CAAP,EAAU;AACVrD,sBAAOkB,IAAP,CACG,qEAAoE6B,OAAQ,mCAAkCM,CAAE,EADnH;AAGD;AACF","sourcesContent":["import path from \"path\"\nimport os from \"os\"\nimport v8 from \"v8\"\nimport {\n  existsSync,\n  mkdtempSync,\n  moveSync, // Note: moveSync over renameSync because /tmp may be on other mount\n  readFileSync,\n  removeSync,\n  writeFileSync,\n  outputFileSync,\n} from \"fs-extra\"\nimport {\n  ICachedReduxState,\n  IGatsbyNode,\n  IGatsbyPage,\n  GatsbyStateKeys,\n} from \"./types\"\nimport { sync as globSync } from \"glob\"\nimport { createContentDigest } from \"gatsby-core-utils\"\nimport report from \"gatsby-cli/lib/reporter\"\nimport { DeepPartial } from \"redux\"\n\nconst getReduxCacheFolder = (): string =>\n  // This is a function for the case that somebody does a process.chdir (#19800)\n  path.join(process.cwd(), `.cache/redux`)\n\nconst getWorkerSlicesFolder = (): string =>\n  // This is a function for the case that somebody does a process.chdir (#19800)\n  path.join(process.cwd(), `.cache/worker`)\n\nfunction reduxSharedFile(dir: string): string {\n  return path.join(dir, `redux.rest.state`)\n}\nfunction reduxChunkedNodesFilePrefix(dir: string): string {\n  return path.join(dir, `redux.node.state_`)\n}\nfunction reduxChunkedPagesFilePrefix(dir: string): string {\n  return path.join(dir, `redux.page.state_`)\n}\nfunction reduxWorkerSlicesPrefix(dir: string): string {\n  return path.join(dir, `redux.worker.slices_`)\n}\n\nexport function readFromCache(\n  slices?: Array<GatsbyStateKeys>,\n  optionalPrefix: string = ``\n): DeepPartial<ICachedReduxState> {\n  // The cache is stored in two steps; the nodes and pages in chunks and the rest\n  // First we revive the rest, then we inject the nodes and pages into that obj (if any)\n  // Each chunk is stored in its own file, this circumvents max buffer lengths\n  // for sites with a _lot_ of content. Since all nodes / pages go into a Map, the order\n  // of reading them is not relevant.\n\n  let cacheFolder = getReduxCacheFolder()\n\n  if (slices) {\n    cacheFolder = getWorkerSlicesFolder()\n\n    return v8.deserialize(\n      readFileSync(\n        reduxWorkerSlicesPrefix(cacheFolder) +\n          `${optionalPrefix}_` +\n          createContentDigest(slices)\n      )\n    )\n  }\n\n  const obj: ICachedReduxState = v8.deserialize(\n    readFileSync(reduxSharedFile(cacheFolder))\n  )\n\n  // Note: at 1M pages, this will be 1M/chunkSize chunks (ie. 1m/10k=100)\n  const nodesChunks = globSync(\n    reduxChunkedNodesFilePrefix(cacheFolder) + `*`\n  ).map(file => v8.deserialize(readFileSync(file)))\n\n  const nodes: Array<[string, IGatsbyNode]> = [].concat(...nodesChunks)\n\n  if (!nodesChunks.length) {\n    report.info(\n      `Cache exists but contains no nodes. There should be at least some nodes available so it seems the cache was corrupted. Disregarding the cache and proceeding as if there was none.`\n    )\n    return {} as DeepPartial<ICachedReduxState>\n  }\n\n  obj.nodes = new Map(nodes)\n\n  // Note: at 1M pages, this will be 1M/chunkSize chunks (ie. 1m/10k=100)\n  const pagesChunks = globSync(\n    reduxChunkedPagesFilePrefix(cacheFolder) + `*`\n  ).map(file => v8.deserialize(readFileSync(file)))\n\n  const pages: Array<[string, IGatsbyPage]> = [].concat(...pagesChunks)\n\n  obj.pages = new Map(pages)\n\n  return obj\n}\n\nexport function guessSafeChunkSize(\n  values: Array<[string, IGatsbyNode]> | Array<[string, IGatsbyPage]>,\n  showMaxSizeWarning: boolean = false\n): number {\n  // Pick a few random elements and measure their size then pick a chunk size\n  // ceiling based on the worst case. Each test takes time so there's trade-off.\n  // This attempts to prevent small sites with very large pages from OOMing.\n  // This heuristic could still fail if it randomly grabs the smallest nodes.\n  // TODO: test a few nodes per each type instead of from all nodes\n\n  const nodesToTest = 11 // Very arbitrary number\n  const valueCount = values.length\n  const step = Math.max(1, Math.ceil(valueCount / nodesToTest))\n  let maxSize = 0\n  for (let i = 0; i < valueCount; i += step) {\n    const size = v8.serialize(values[i]).length\n    maxSize = Math.max(size, maxSize)\n  }\n\n  // Sends a warning once if any of the chunkSizes exceeds approx 500kb limit\n  if (showMaxSizeWarning && maxSize > 500000) {\n    report.warn(\n      `The size of at least one page context chunk exceeded 500kb, which could lead to degraded performance. Consider putting less data in the page context.`\n    )\n  }\n\n  // Max size of a Buffer is 2gb (yeah, we're assuming 64bit system)\n  // https://stackoverflow.com/questions/8974375/whats-the-maximum-size-of-a-node-js-buffer\n  // Use 1.5gb as the target ceiling, allowing for some margin of error\n  return Math.floor((1.5 * 1024 * 1024 * 1024) / maxSize)\n}\n\nfunction prepareCacheFolder(\n  targetDir: string,\n  contents: DeepPartial<ICachedReduxState>\n): void {\n  // Temporarily save the nodes and pages and remove them from the main redux store\n  // This prevents an OOM when the page nodes collectively contain to much data\n  const nodesMap = contents.nodes\n  contents.nodes = undefined\n\n  const pagesMap = contents.pages\n  contents.pages = undefined\n\n  writeFileSync(reduxSharedFile(targetDir), v8.serialize(contents))\n\n  // Now restore them on the redux store\n  contents.nodes = nodesMap\n  contents.pages = pagesMap\n\n  if (nodesMap) {\n    if (nodesMap.size === 0 && process.env.GATSBY_EXPERIMENTAL_LMDB_STORE) {\n      // Nodes are actually stored in LMDB.\n      //  But we need at least one node in redux state to workaround the warning above:\n      //  \"Cache exists but contains no nodes...\" (when loading cache).\n      // Sadly, cannot rely on GATSBY_EXPERIMENTAL_LMDB_STORE env variable at cache load time\n      //  because it is not initialized at this point (when set via flags in config)\n      const dummyNode: IGatsbyNode = {\n        id: `dummy-node-id`,\n        parent: ``,\n        children: [],\n        internal: {\n          type: `DummyNode`,\n          contentDigest: `dummy-node`,\n          counter: 0,\n          owner: ``,\n        },\n        __gatsby_resolved: {},\n        fields: [],\n      }\n      nodesMap.set(dummyNode.id, dummyNode)\n    }\n    // Now store the nodes separately, chunk size determined by a heuristic\n    const values: Array<[string, IGatsbyNode]> = [...nodesMap.entries()]\n    const chunkSize = guessSafeChunkSize(values)\n    const chunks = Math.ceil(values.length / chunkSize)\n\n    for (let i = 0; i < chunks; ++i) {\n      writeFileSync(\n        reduxChunkedNodesFilePrefix(targetDir) + i,\n        v8.serialize(values.slice(i * chunkSize, i * chunkSize + chunkSize))\n      )\n    }\n  }\n\n  if (pagesMap) {\n    // Now store the nodes separately, chunk size determined by a heuristic\n    const values: Array<[string, IGatsbyPage]> = [...pagesMap.entries()]\n    const chunkSize = guessSafeChunkSize(values, true)\n    const chunks = Math.ceil(values.length / chunkSize)\n\n    for (let i = 0; i < chunks; ++i) {\n      writeFileSync(\n        reduxChunkedPagesFilePrefix(targetDir) + i,\n        v8.serialize(values.slice(i * chunkSize, i * chunkSize + chunkSize))\n      )\n    }\n  }\n}\n\nfunction safelyRenameToBak(cacheFolder: string): string {\n  // Basically try to work around the potential of previous renamed caches\n  // not being removed for whatever reason. _That_ should not be a blocker.\n  const tmpSuffix = `.bak`\n  let suffixCounter = 0\n  let bakName = cacheFolder + tmpSuffix // Start without number\n\n  while (existsSync(bakName)) {\n    ++suffixCounter\n    bakName = cacheFolder + tmpSuffix + suffixCounter\n  }\n  moveSync(cacheFolder, bakName)\n\n  return bakName\n}\n\nexport function writeToCache(\n  contents: DeepPartial<ICachedReduxState>,\n  slices?: Array<GatsbyStateKeys>,\n  optionalPrefix: string = ``\n): void {\n  // Writing the \"slices\" also to the \"redux\" folder introduces subtle bugs when\n  // e.g. the whole folder gets replaced some \"slices\" are lost\n  // Thus they get written to dedicated \"worker\" folder\n  if (slices) {\n    const cacheFolder = getWorkerSlicesFolder()\n\n    outputFileSync(\n      reduxWorkerSlicesPrefix(cacheFolder) +\n        `${optionalPrefix}_` +\n        createContentDigest(slices),\n      v8.serialize(contents)\n    )\n    return\n  }\n\n  // Note: this should be a transactional operation. So work in a tmp dir and\n  // make sure the cache cannot be left in a corruptable state due to errors.\n\n  const tmpDir = mkdtempSync(path.join(os.tmpdir(), `reduxcache`)) // linux / windows\n\n  prepareCacheFolder(tmpDir, contents)\n\n  // Replace old cache folder with new. If the first rename fails, the cache\n  // is just stale. If the second rename fails, the cache is empty. In either\n  // case the cache is not left in a corrupt state.\n\n  const reduxCacheFolder = getReduxCacheFolder()\n\n  let bakName = ``\n  if (existsSync(reduxCacheFolder)) {\n    // Don't drop until after swapping over (renaming is less likely to fail)\n    bakName = safelyRenameToBak(reduxCacheFolder)\n  }\n\n  // The redux cache folder should now not exist so we can rename our tmp to it\n  moveSync(tmpDir, reduxCacheFolder)\n\n  // Now try to yolorimraf the old cache folder\n  try {\n    if (bakName !== ``) {\n      removeSync(bakName)\n    }\n  } catch (e) {\n    report.warn(\n      `Non-fatal: Deleting the old cache folder failed, left behind in \\`${bakName}\\`. Rimraf reported this error: ${e}`\n    )\n  }\n}\n"],"file":"persist.js"}